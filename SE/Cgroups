cgroups(Control Groups) 是 linux 内核提供的一种机制，这种机制可以根据需求把一系列系统任务及其子任务整合(或分隔)
到按资源划分等级的不同组内，从而为系统资源管理提供一个统一的框架。简单说，cgroups 可以限制、记录任务组所使用的物理资源。
本质上来说，cgroups 是内核附加在程序上的一系列钩子(hook)，通过程序运行时对资源的调度触发相应的钩子以达到资源追踪和限制的目的。

实现 cgroups 的主要目的是为不同用户层面的资源管理提供一个统一化的接口。从单个任务的资源控制到操作系统层面的虚拟化，
cgroups 提供了四大功能：

资源限制：cgroups 可以对任务是要的资源总额进行限制。（超过就将任务暂停或者杀死）
比如设定任务运行时使用的内存上限，一旦超出就发 OOM(out of memory)。
优先级分配：通过分配的 CPU 时间片数量和磁盘 IO 带宽，实际上就等同于控制了任务运行的优先级。
资源统计：cgoups 可以统计系统的资源使用量，比如 CPU 使用时长、内存用量等。这个功能非常适合当前云端产品按使用量计费的方式。
任务控制：cgroups 可以对任务执行挂起、恢复等操作。


任务(task): 在cgroup中，任务就是一个进程。
控制组(control group): cgroup的资源控制是以控制组的方式实现，控制组指明了资源的配额限制。进程可以加入到某个控制组，
也可以迁移到另一个控制组。
层级(hierarchy): 控制组有层级关系，类似树的结构，子节点的控制组继承父控制组的属性(资源配额、限制等)。
子系统(subsystem): 一个子系统其实就是一种资源的控制器，比如memory子系统可以控制进程内存的使用。
子系统需要加入到某个层级，然后该层级的所有控制组，均受到这个子系统的控制。

概念间的关系：

子系统可以依附多个层级，当且仅当这些层级没有其他的子系统，比如两个层级同时只有一个cpu子系统，是可以的。
一个层级可以附加多个子系统。
一个任务可以是多个cgroup的成员，但这些cgroup必须位于不同的层级。
子进程自动成为父进程cgroup的成员，可按需求将子进程移到不同的cgroup中。
两个任务组成了一个 Task Group，并使用了 CPU 和 Memory 两个子系统的 cgroup，用于控制 CPU 和 MEM 的资源隔离。






内核将进程分为两种级别，普通进程和实时进程，实时进程的优先级高于普通进程，另外他们的调度策略也不同。
如果一个进程是实时进程，只要它是可执行状态的，内核就一直让它执行，以尽可能满足它对cpu的需要，直到进程执行完成，睡眠或退出（不可执行状态）。
如果有多个进程一直处于可执行状态，则内核会先满足优先级最高的实时进程对cpu的需求，直到这个进程执行完成变成非可执行状态。
内核中通过/proc/sys/kernel/sched_rt_runtime_us和/proc/sys/kernel/sched_rt_period_us两个参数来控制在sched_rt_period_us为周期的时间内，
实时进程最多只能运行sched_rt_runtime_us这么多时间，这样留给普通进程一定的运行时间。
如果有多个相同优先级的实时进程处于可执行状态，有两种调度策略可以选择
SCHED_FIFO：先进先出。直到先被执行的进程变为非可执行状态，后来的进程才被调度执行。在这种策略下，先来的进程可以执行sched_yield系统调用，自愿放弃CPU，以让权给后来的进程；
SCHED_RR：轮转调度。内核为实时进程分配时间片，在时间片用完时，让下一个进程使用CPU



blkio 对块设备的 IO 进行限制。。有两种限制方式：权重和上限，权重是给不同的应用一个权重值，按百分比使用IO资源，上限是控制应用读写速率的最大值。
cpu 限制 CPU 时间片的分配，与 cpuacct 挂载在同一目录。
cpuacct 生成 cgroup 中的任务占用 CPU 资源的报告，与 cpu 挂载在同一目录。
cpuset 给 cgroup 中的任务分配独立的 CPU(多处理器系统) 和内存节点。
devices 允许或禁止 cgroup 中的任务访问设备。
freezer 暂停/恢复 cgroup 中的任务。
hugetlb 限制使用的内存页数量。
memory 对 cgroup 中的任务的可用内存进行限制，并自动生成资源占用报告。
net_cls 使用等级识别符（classid）标记网络数据包，这让 Linux 流量控制器（tc 指令）可以识别来自特定 cgroup 任务的数据包，并进行网络限制。
net_prio 允许基于 cgroup 设置网络流量(netowork traffic)的优先级。
perf_event 允许使用 perf 工具来监控 cgroup。
pids 限制任务的数量。

Linux通过文件的方式，将cgroups的功能和配置暴露给用户，这得益于Linux的虚拟文件系统（VFS）。
VFS将具体文件系统的细节隐藏起来，给用户态提供一个统一的文件系统API接口，cgroups和VFS之间的链接部分，称之为cgroups文件系统。




OOM

/proc/sys/vm/panic_on_oom
当该参数等于0的时候，表示启动OOM killer（默认为0）。当该参数不为0的时候，表示无论是哪一种情况，都强制进入kernel panic（宕机）。
OOM killer
选择一个或者几个最“适合”的进程，启动OOM killer，干掉那些选中的进程，释放内存
挑选规则：1.哪个进程触发的OOM 2.哪个进程占用内存最多
前提：考虑是否用户空间进程（不能杀内核线程）、是否unkillable task（例如init进程就不能杀），用户空间是否通过设定参数（oom_score_adj）阻止kill该task。
如果万事俱备，那么就调用oom_kill_process干掉当前进程。
打分：/proc/PID/oom_score_adj　范围 -1000——1000。负值表示要在实际打分值上减去一个折扣，正值表示要惩罚该task，也就是增加该进程的oom_score
得分：/proc/PID/oom_score
值为0，不会被杀死，值越大，越容易被OOM killer杀死


1.Kernel-2.6.26之前版本的oomkiller算法不够精确，RHEL6.x版本的2.6.32可以解决这个问题。
2.子进程会继承父进程的oom_adj。
3.OOM不适合于解决内存泄漏(Memory leak)的问题。
4.有时free查看还有充足的内存，但还是会触发OOM，是因为该进程可能占用了特殊的内存地址空间。


查看最高的打分：
#!/bin/bash
for proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do
printf "%2d %5d %s\n" \
"$(cat $proc/oom_score)" \
"$(basename $proc)" \
"$(cat $proc/cmdline | tr '\0' ' ' | head -c 50)"
done 2>/dev/null | sort -nr | head -n 10
